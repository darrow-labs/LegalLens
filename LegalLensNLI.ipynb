{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "python",
       "nbconvert_exporter": "python",
       "pygments_lexer": "ipython3",
       "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc2gcBOudc59"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.31.0 datasets==2.15.0 trl==0.7.10 peft==0.7.1 bitsandbytes==0.42.0 flash-attn==2.5.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import pickle\n",
        "import datasets\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import LoraConfig\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")"
      ],
      "metadata": {
        "id": "x8oaVCUFtNI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_ckpt = \"tiiuae/falcon-7b\"\n",
        "\n",
        "legal_type = \"consumer_protection\"\n",
        "\n",
        "experiment_run = 1\n",
        "lora_r = 64\n",
        "epochs = 20\n",
        "dropout = 0.25\n",
        "\n",
        "results_dir = f\"experiments/falcon_nli-{legal_type}_epochs-{epochs}_rank-{lora_r}_dropout-{dropout}_expRun-{str(experiment_run)}\""
      ],
      "metadata": {
        "id": "JI-ywPXT3Vh3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_PROMPT = \"\"\"###Premise:{premise} ###Hypothesis:{hypothesis} ###Label:{label}\"\"\"\n",
        "INFERENCE_PROMPT = \"\"\"###Premise:{premise} ###Hypothesis:{hypothesis} ###Label:\"\"\""
      ],
      "metadata": {
        "id": "56mV39hVdsky"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_nli_data(legal_type: str) -> pd.DataFrame:\n",
        "    justice_lens_dataset = datasets.load_dataset(\"darrow-ai/LegalLensNLI\") # This is a snippet from our dataset\n",
        "\n",
        "    df = (\n",
        "        justice_lens_dataset[\"train\"]\n",
        "        .filter(lambda example: example[\"legal_act\"] != legal_type)\n",
        "        .to_pandas()\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_test_nli_data(legal_type: str) -> pd.DataFrame:\n",
        "    justice_lens_dataset = datasets.load_dataset(\"darrow-ai/LegalLensNLI\") # This is a snippet from our dataset\n",
        "\n",
        "    df = (\n",
        "        justice_lens_dataset[\"train\"]\n",
        "        .filter(lambda example: example[\"legal_act\"] == legal_type)\n",
        "        .to_pandas()\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_instruction(premise, hypothesis, label, is_train=False):\n",
        "    if is_train:\n",
        "        instruction = TRAINING_PROMPT.format(\n",
        "            premise=premise, hypothesis=hypothesis, label=label\n",
        "        )\n",
        "    else:\n",
        "        instruction = INFERENCE_PROMPT.format(\n",
        "            premise=premise,\n",
        "            hypothesis=hypothesis,\n",
        "        )\n",
        "\n",
        "    return instruction\n",
        "\n",
        "\n",
        "def get_instructions(df, is_train=False):\n",
        "    instructions = []\n",
        "    for idx, row in df.iterrows():\n",
        "        premise = row[\"premise\"]\n",
        "        hypothesis = row[\"hypothesis\"]\n",
        "        label = row[\"label\"]\n",
        "\n",
        "        prompt = prepare_instruction(premise, hypothesis, label, is_train=is_train)\n",
        "\n",
        "        instructions.append(prompt)\n",
        "\n",
        "    return instructions\n",
        "\n",
        "\n",
        "def predict_one_sample(prompt, model, tokenizer):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=250,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            temperature=0.01,\n",
        "        )\n",
        "        output = tokenizer.batch_decode(\n",
        "            outputs.detach().cpu().numpy(), skip_special_tokens=True\n",
        "        )[0][len(prompt) :]\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "6WLCMM0w3W_K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_train_nli_data(legal_type=legal_type)\n",
        "train_instructions = get_instructions(df_train, is_train=True)\n",
        "train_dataset = datasets.Dataset.from_pandas(\n",
        "    pd.DataFrame(data={\"instructions\": train_instructions})\n",
        ")\n",
        "\n",
        "df_test = get_test_nli_data(legal_type=legal_type)\n",
        "test_instructions = get_instructions(df_test, is_train=False)\n",
        "test_dataset = datasets.Dataset.from_pandas(\n",
        "    pd.DataFrame(data={\"instructions\": test_instructions})\n",
        ")"
      ],
      "metadata": {
        "id": "hHBoqH3T2xM_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_flash_attention = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_ckpt)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Getting PEFT method\")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_alpha=32,\n",
        "    r=lora_r,\n",
        "    lora_dropout=dropout,\n",
        "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_name_or_path = pretrained_ckpt\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_ckpt,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Define training args\n",
        "training_args = TrainingArguments(\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    output_dir=results_dir,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=epochs,\n",
        "    logging_dir=f\"{results_dir}/logs\",\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        ")\n",
        "\n",
        "print(f\"training_args = {training_args}\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"instructions\",\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "train_loss = trainer_stats.training_loss\n",
        "print(f\"Training loss:{train_loss}\")\n",
        "\n",
        "peft_model_id = f\"{results_dir}/assets\"\n",
        "trainer.model.save_pretrained(peft_model_id)\n",
        "tokenizer.save_pretrained(peft_model_id)\n",
        "\n",
        "with open(f\"{results_dir}/results.pkl\", \"wb\") as handle:\n",
        "    run_result = [\n",
        "        epochs,\n",
        "        lora_r,\n",
        "        dropout,\n",
        "        train_loss,\n",
        "    ]\n",
        "    pickle.dump(run_result, handle)\n",
        "print(\"Experiment over\")"
      ],
      "metadata": {
        "id": "SKc-buH8tX_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = os.path.join(results_dir, \"assets\")\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Do few-shot prompting\n",
        "responses = []\n",
        "labels = df_test[\"label\"].to_list()\n",
        "\n",
        "save_dir = os.path.join(results_dir, \"inference\")\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "for prompt in tqdm(test_instructions):\n",
        "    response = predict_one_sample(prompt, model, tokenizer)\n",
        "    responses.append(response)\n",
        "\n",
        "\n",
        "metrics = {\n",
        "    \"micro_f1\": f1_score(labels, responses, average=\"micro\"),\n",
        "    \"macro_f1\": f1_score(labels, responses, average=\"macro\"),\n",
        "    \"micro_precision\": precision_score(labels, responses, average=\"micro\"),\n",
        "    \"micro_recall\": recall_score(labels, responses, average=\"micro\"),\n",
        "    \"macro_precision\": precision_score(labels, responses, average=\"macro\"),\n",
        "    \"macro_recall\": recall_score(labels, responses, average=\"macro\"),\n",
        "    \"accuracy\": accuracy_score(labels, responses),\n",
        "}\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "UF56V-Ri2F6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpf7Yn3ShRat"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
